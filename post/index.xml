<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on moskomule log</title>
    <link>https://mosko.tokyo/post/</link>
    <description>Recent content in Posts on moskomule log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <copyright>Written by Ryuichiro Hataya</copyright>
    <lastBuildDate>Mon, 25 Dec 2017 15:43:55 +0900</lastBuildDate>
    
	<atom:link href="https://mosko.tokyo/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>NIPS2017まとめのまとめ</title>
      <link>https://mosko.tokyo/post/nips2017/</link>
      <pubDate>Mon, 25 Dec 2017 15:43:55 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/nips2017/</guid>
      <description>After being extremely popular in the early 1990s, neural networks have fallen out of favor in research in the last 5 years. In 2000, it was even pointed out by the organizers of the Neural Information Processing System (NIPS) conference that the term “neural networks” in the submission title was negatively correlated with acceptance. In contrast, positive correlations were made with support vector machines (SVMs), Bayesian networks, and variational methods.</description>
    </item>
    
    <item>
      <title>PyTorchでDQNを実装した</title>
      <link>https://mosko.tokyo/post/pytorch-dqn/</link>
      <pubDate>Fri, 01 Dec 2017 15:10:30 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/pytorch-dqn/</guid>
      <description>はじめに DQN(Deep Q Network)は Minh et al. 20151（以下論文）で登場した深層強化学習の先駆けです．Atariのゲームで非常に高い得点を修めるというパフォーマンスで有名になりました．
  9月頃に強化学習の勉強をした際に実装してみたのですが，一向に学習が進まず放置していたのですが，最近Implementing the Deep Q-Network 2を読み再開してみたところ，動いてしまったので，この記事を書くことになりました．
今回の実装はこちらにあります．
強化学習とは David Silver先生に聞きましょう．ただしこの講義では深層強化学習は扱われていません．
  Deep Q-Networkとは 論文を読みましょう．Q-Learningの応用で，複雑ではありませんが，学習を安定させるための工夫が各所にあるので見逃すと動かないようです．
 DQNの学習アルゴリズム．論文より．   実装について 画像の処理 DQNではAtariのゲームの画像をグレースケールにしてスタックするなどの処理がありますが，このあたりは各アルゴリズムをTensorFlowで実装し，公開しているOpen AI baselinesを一部変更して用いています．
 OpenCV2をPillowに変更した 画像のスタックの仕方をPyTorchに合わせて変更した．  また今回の改良ではtensorboard-pytorchを導入して，入力画像が正しいかを確認できるようにしました．
 ネットワーク Deepとは言えない気がしますが論文通りの構成です．何か工夫すると多少変わるのかもしれません．
class DQN(nn.Module): def __init__(self, output_size: int): super(DQN, self).__init__() self.feature = nn.Sequential( nn.Conv2d(4, 32, kernel_size=8, stride=4), nn.ReLU(inplace=True), nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(inplace=True), nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU(inplace=True)) self.fc = nn.</description>
    </item>
    
    <item>
      <title>Dynamic Routing Between Capsules</title>
      <link>https://mosko.tokyo/post/on-capusels/</link>
      <pubDate>Mon, 13 Nov 2017 18:57:37 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/on-capusels/</guid>
      <description>はじめに ディープラーニングの帝王1Geoffrey Hinton先生が20年来温め続けてきたというCapsules Network(CapsNet)を実装し成果を出して論文を出しました(Dynamic Routing Between Capsules)．この記事ではこのCapsNetについて簡単にまとめます．
Hinton先生の業績は並べても感嘆するばかりで仕方ないのですが，帝王でおはしますと同時に認知心理学者でもあります．このCapsules Networkも脳のコラム構造を真似ることでCNNよりも人間の認知機能に近い認識器をつくることが目標です．
本論文の要点は以下の通りです．
 今まで重みと入力の内積というスカラーの塊であった層を，Capsulesという「ベクトルの塊」とすることで「姿勢」などの情報を扱えるようにした． 層間のCapsulesの結合(routing)を学習できるようにした． 実際にCapsulesを用いた3層のネットワーク(CapsNet)を構築した． CapsNetはデータ拡張なしにMNISTのテストエラー0.25%を達成した． 文字が重なったMNISTに対してもAttentionを用いた複雑なネットワークと同等以上の性能を発揮した．  CNNの問題点 さて，Hinton先生は度々（AlexNet登場以前から！）畳み込みニューラルネットワーク(CNN)の問題点を指摘しており，Capsulesはその解決策のひとつでもあります．
各所で&amp;rdquo;What is wrong with Convolutional Neural Nets&amp;rdquo;という題の講演を行っているようです．（受講者のノート，まとめ）．
  LeNetにはじまるCNNでは，ゆがみや並行移動に対する不変性を得るためにpooling(sub-sampling)が行われます．これは却って，位置に関する重要な情報を取り去って特徴を学習していることに他なりません．
従ってCNNでは，以下の図のように位置関係を無視して「目」や「口」といった個々の特徴のみで画像を認識することとなり，左図のようなそれぞれの要素がばらばらのものも右図のような顔であるものも同じように認識してしまいます．
一方で人間などの認知では目鼻と顔の階層関係から顔であることを認識しているので，両者を区別することができます．言及はされていませんがイラストや顔文字，さらに一部が隠れた顔をも顔と認識できるのもこのお陰でしょう．
 CNNは位置を無視してしまうので左の画像も右の画像も同じ特徴を持った「人」であると認識してしまう．   またCNNでは同じものに対しても，僅かな差異を学習するために大量のデータとデータ拡張を必要とします．
以下に「傾いた人の顔」を示しましたが，CNNでこれを「人」と認識するためには，そのような多くのデータによる学習が不可欠です．更に，これによって認識できるようになって右の画像が「人」であることは分かっても，「傾いた人の顔」であるかどうかは分かりません．
一方で人間などの認知ではたとえ「傾いた人の顔」を見たことがなくても，例えば鼻の傾きから座標を割り出して画像が「人の顔」であり更に「傾いている」ことが分かります．
 CNNでは右の画像が「人」であることを学習するのに多くのデータとデータ拡張を要するが，それでも「傾いた人の顔」であることは分からない．   “But my CAPSULES can do” 一方のCapsNetでは位置や姿勢などの情報をcapsuleというベクトルの形で保持し，capsule同士が階層的な「構文木」をなすように結合の仕方（routing）を学習します．Capsulesは脳にあるコラム構造に影響を受けているようです．
Capsules 各capsuleには階層によって異なりますが「目」や「顔」などが対応します2．capsuleはベクトルで，長さがそのcapsuleに対応するものが存在する確率であり，各要素が位置や姿勢を表します．
$l+1$層目のcapsule $j$は$l$層目のcapsulesの出力に，以下で説明するroutingを施した入力$s_j$を受けます．これはそのままでは上記の条件を満たさないので以下のsquashing（押し潰し）によって，$[0,1)$に閉じ込めます．
\[v_j=\frac{|s_j|^2}{1+|s_j|^2}\frac{s_j}{|s_j}\]
 $l,l&amp;#43;1$層での記号の関係．   routing 「目」のcapsuleと「顔」のcapsuleとは結びつくべきですが，「花」のcapsuleとは結びつくことはありません．これを学習するのがroutingであり，ここでは特に本論文で提案されているdynamic routingを扱います．このほかにEMアルゴリズムを用いた手法も何者かによって提案されています．
$l+1$層目のcapsule $j$が$l$層目のcapsules $i$の出力を受けます．先ほどのroutingされた$j$への入力$s_j$は以下によって計算されます．capsule $i$の出力を$u_i(=v_i)$とします．
\[s_j=\sum_i c_{ij}\hat{u}_{j|i}~~(\hat{u}_{j|i}=W_{ij}u_i)\]
ただし$c_{ij}$は$b_{ij}\leftarrow b_{ij}+\hat{u}_{j|i}\cdot v_j$と逐次的に更新される$b_{ij}$によって</description>
    </item>
    
    <item>
      <title>研究のためのDocker入門</title>
      <link>https://mosko.tokyo/post/docker_for_research/</link>
      <pubDate>Thu, 19 Oct 2017 22:42:01 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/docker_for_research/</guid>
      <description>はじめに 「TensorFlowのCustomOpsが古いGCCでしかコンパイルできず詰んだ」「先輩から引き継いだコードを動かす環境構築が面倒」といった経験はないでしょうか．この記事ではDockerを使うことで環境依存/環境汚染の問題を減らすことを考えていきます．
1年ほど前からDockerの存在は知っていましたが，いざはじめようと「Docker入門」のような記事を読むとUbuntuのイメージをプルしてコンテナを作ってお疲れ様でした，という感じであまり御利益が分かりませんでした．
先日絶対に挫折しない！オープンソースソフトウェア「Docker」入門編 #04という記事でDockerfileを作って環境を構築する話を読んで少し得心できました．どこでも同一のコマンドで同一の環境を，ホストの環境を汚さずに作れるので非常に便利です．
インストール等 DockerCEをダウンロードしてインストールします．環境によってはDocker toolboxを利用する必要があります．
Linuxの場合はインストール後に
$ sudo groupadd docker $ sudo usermod -aG docker $USER $ logout # login later  を行います．インストールが成功していれば以下のようになるはずです．
$ docker run --rm hello-world ... Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &amp;quot;hello-world&amp;quot; image from the Docker Hub.</description>
    </item>
    
    <item>
      <title>Double Backpropagationについて</title>
      <link>https://mosko.tokyo/post/double-backprop/</link>
      <pubDate>Sun, 01 Oct 2017 19:40:24 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/double-backprop/</guid>
      <description>はじめに PyTorch v0.2では&amp;rdquo;Higher order gradients&amp;rdquo; (double backpropagation)がサポートされました．Chainerもv3においてこれがサポートされます．今回Chainer Meetupの資料を読んで雰囲気が分かったのでまとめました．
 Comparison of deep learning frameworks from a viewpoint of double backpropagation  Chainer v3  筆者は長くdouble backpropagationという名称から
\[\mathrm{loss}\longrightarrow \frac{\partial^2 \mathrm{loss}}{\partial x_i \partial x_j} \]
と思い込んでいました．そう思っているのでdocumentを読んでもいまいちよく分からない．ところが上に挙げた資料では，そうではなくて
\[\mathrm{loss}=g(f(x), \frac{\partial f(x)}{\partial x})\]
のような場合にも計算が出来る，ということなのだということが説明されていて救われました．
PyTorchの例 これで以上，でもよいのですが，PyTorchでの例を．
$x=1, y=x^3, z=y^2+\frac{dy}{dx}$をとして，$\frac{dz}{dx}|_{x=1}$を求めます．
&amp;gt;&amp;gt;&amp;gt; x = Variable(torch.Tensor([1]), requires_grad=True) &amp;gt;&amp;gt;&amp;gt; y = x ** 3 &amp;gt;&amp;gt;&amp;gt; grad_y, = autograd.grad(y, x, create_graph=True) &amp;gt;&amp;gt;&amp;gt; (grad_y + y ** 2).backward() &amp;gt;&amp;gt;&amp;gt; x.grad Variable containing: 12 [torch.</description>
    </item>
    
    <item>
      <title>NNablaの静的・動的計算グラフの比較</title>
      <link>https://mosko.tokyo/post/nnabla/</link>
      <pubDate>Tue, 25 Jul 2017 15:37:12 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/nnabla/</guid>
      <description>はじめに NNablaはSonyによるニューラルネットワークライブラリです．特徴としては公式ページにあるように動的計算グラフと静的計算グラフの双方をサポートすること，PythonとC++のAPIが用意されていること，Xperia Earなどの小型端末をはじめ，さまざまな機器の上で動き実際に利用されていることなどがあります．
加えてレイヤーがParametric Functionsという名前で表されていること，バイナリレイヤーが用意されていること，学習のモニタ機能が充実していること(Monitors)なども特徴と言えるのではないでしょうか．逆にRNNレイヤーなどは用意されていません．
最近Python3に対応し使えるようになったこともあり，Sonyに愛着があったので試してみました．MNISTをやるだけです．
インストール LinuxとWindowsではpip install -U nnablaで導入できるのですが，macOSではソースからのビルドの必要があります(2017/07/26現在)．
brew install protoc git clone https://github.com/sony/nnabla cd nnabla sudo pip install -U -r python/setup_requirements.txt sudo pip install -U -r python/requirements.txt mkdir build cd build export MACOSX_DEPLOYMENT_TARGET=10.9 cmake ../ make -j 16 cp lib/libnnabla.dylib /usr/local/lib/ cd dist sudo pip install -U &amp;lt;build-wheel-file&amp;gt;.whl  モデル LeNetの活性化函数をreluに変えたものを使います．とりあえずPyTorch風に書いています．nnabla.get_parameters()はparameter_scope内を参照するようですので適宜設定します．
import nnabla from nnabla import Variable import nnabla.functions as F import nnabla.parametric_functions as PF from nnabla import solvers class Lenet(object): def __init__(self): self.</description>
    </item>
    
    <item>
      <title>最適化手法について—SGDからYellowFinまで—</title>
      <link>https://mosko.tokyo/post/optimization2/</link>
      <pubDate>Wed, 12 Jul 2017 12:37:12 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/optimization2/</guid>
      <description>はじめに 前回の記事に，「ベンチマーク函数があるよ」というフィードバックを頂きました．test functions for optimizationというWikipediaの記事にまとまっていたので，今回はこちらにあるRosenbrock function($f_{R}=100(y-x^2)^2+(x-1)^2$,大域解$f_{R}(1,1)=0$)を使います．Rosenbrock函数には大域解を含む広い濠があって大域解が見つけにくいのが特徴です．
一般に勾配法の更新方法はバッチ更新と呼ばれます．つまり勾配降下法であれば学習事例$n=1,2,\ldots,N$に対して個々の誤差函数$E_n$の和
\[E(x)=\sum_n E_n(x)\]
について$x\gets x-\alpha\nabla E$と更新していました．
確率的な(stochastic)では$n\in\{1,\ldots,N\}$を乱択して，または逐次的な(online)勾配降下法では$n=1,2,\ldots,$によって$x\gets x-\alpha\nabla E_n(x)$により更新を行います．
また，ディープラーニングの文脈では$B\subset\{1,\ldots,N\}$によって$x\gets x-\frac{\alpha}{|B|}\sum_{i\in B}\nabla E_i(x)$を更新するミニバッチ更新がしばしば用いられます．ミニバッチの大きさ$|B|$は32,64,128などがよく用いられるように思われます．この大きさが大きいほど計算は速くなりますが，あまり大きくない方が汎化性能が上がるという話もあります1．
今回紹介する手法は主にディープラーニングの文脈で用いられるため，ミニバッチ更新を行うことでよりよい性能が出せる可能性がありますが，記事中では可視化の都合もあり$x\gets x-\alpha d$によって更新を行います．
なお，表記にはこの記事特有のものも含まれていますので，その点にはお気をつけ下さい．
SGD(Stochastic Gradient Descent) 基本的に勾配降下法と同一で，以下により更新を行います．
\[x_{k+1}=x_{k}-\alpha_k\nabla f(x_{k})\]
$\alpha$の調整には段階的に$\alpha$を小さくするstep decay，$\alpha_k=\alpha_0 e^{-rk}$とするexponential decay，$\alpha_k=\frac{\alpha_0}{1+rk}$とする1/k decayなどの手法がありますが，職人の勘による調整法も多いようです．
 初期位置(-0.7, -0.7),(0, 2.5),(-1, 2)からのSGDによる1000ステップの移動の軌跡．黄色の点が大域解(1,1)．   Momentum SGDは$\nabla f(x)$が0に近い時に更新ができなくなるという問題がありました．Momentum法では
\[\begin{aligned} v_{k+1} &amp;amp;= \mu v_k-\alpha\nabla f(x_k)~,v_0=0 \cr\cr x_{k+1} &amp;amp;= x_k+v_{k+1} \end{aligned}\]
によって更新を行うことでこの問題を解決しています．またSGDは各点の勾配変化に敏感でしたが，以前の状態を引き継ぐ慣性力のようなmomentum term$v$を用いることで些細な変化に対しての感度を低下させているとみることもできます．
 初期位置(-0.7, -0.7),(0, 2.5),(-1, 2)からのmomentumつきのSGDによる1000ステップの移動の軌跡．黄色の点が大域解(1,1)．   Nestrov Accelerated Gradient Momentum法を改良し，$v_{k+1}$の更新に$x_k$よりも$x_{k+1}$に近いと期待される$x_k+\mu v_k$を用いたのがNestrov Accelerated Gradientです．</description>
    </item>
    
    <item>
      <title>最適化手法についてー勾配法，ニュートン法，準ニュートン法などー</title>
      <link>https://mosko.tokyo/post/optimization/</link>
      <pubDate>Sun, 09 Jul 2017 12:37:12 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/optimization/</guid>
      <description>以前Eve optimizerの実装を行ったのですが，肝心の非線型函数の最適化手法について知らなかったので調べました．
はじめに 最適化に関して，微分を用いない手法としてはランダム法やシンプレックス法がありますが，今回は微分を用いて反復的に解に近づく方法である反復法について述べます．
なお今回可視化に際して利用した函数は$(x+1)x(x-1)(x-3)+y^2+xy$で，2つの局所解と1つの鞍点を持ちます．
反復法 反復法は函数$f(x)$について，
\[\begin{aligned} d_k &amp;amp;= -H_k\nabla f(x_k) \cr\cr x_{k+1} &amp;amp;= x_{k}+\alpha_{k}d_{k} \end{aligned}\]
によって位置を更新しながら列$(x_k)_{k\in\mathscr{N}}$を局所解または大局解$x^{\star}$に近づけていく手法です．
実際は$k$は有限なので，適当な回数で打ち切ったり，$|x_{k+1}-x_{k}|&amp;lt;\epsilon$で中止したりします．
$x$から$x+\delta x$へと移動した際の$f$の変化$\delta f(x)$とします．$\delta f(x)=\delta x\nabla f(x)$は$\delta x$と$\nabla f(x)$とが並行の時に最大となりますので，$\nabla f(x)$は$x$において$f(x)$の値が最も変化する方向です．そのため局所的には$-\nabla f(x)$に進むのが望ましいですが，それが全体として望ましいとは必ずしもいえません．反復法ではこの方向に適当な$H_k$をかけて調整した上で，順次移動していきます．
$a_k$は学習率，ステップ幅などと呼ばれ一回の更新で進む量を表します．$\alpha_k$にはヒューリスティックな更新方法もありますが($\alpha_k \sim \frac{1}{\sqrt{k}}$など)，Armijoの基準やWolfeの基準といったより客観的な指標もあります．
\[\begin{aligned} f(x_k+\alpha_k d_k) &amp;amp;\le f(x_k)+c_1\alpha_k\nabla f(x_k)^{\top}d_k \cr\cr \nabla f(x_k+\alpha_k d_k)^{\top}d_k &amp;amp;\ge c_2\nabla f(x_k)d_k \end{aligned}\]
上の式がArmijoの基準で，2つ合わせるとWolfeの基準となります．$\alpha_k$がこの範囲に収まるように変化させていきます(line search method1)．
勾配降下法 勾配降下法(gradient descent method)，または最急降下法(steepest descent method)は$H_k=I$とする手法です．すなわち
\[x_{k+1}=x_{k}-\alpha_k\nabla f(x_{k})\]
によって更新していきます．最急方向に適当な学習率$\alpha_k$を乗じて進んでいくので「直感的には合理的」2ですが，収束が遅く，適切な学習率の調整が難しいなど，実際の性能はあまりよくありません．
  $\alpha_k=0.1$で100回の更新を行った際の$x_k$の軌跡    $\alpha_k=0.01$で100回の更新を行った際の$x_k$の軌跡    $\alpha_k=0.001$で100回の更新を行った際の$x_k$の軌跡    最急降下法に対してWolfe基準を用いて学習率を調整した際の$x_k$の軌跡   ニュートン法 ニュートン法，またはニュートン・ラフソン法(Newton-Raphson method)は$H_k$に$(\nabla^2 f(x))^{-1}$，つまりHessianの逆行列を用いた手法です．これはニュートンの近似法3によって$f(x)$の極値を求めていると見ることができます．ニュートンの近似法は函数$g(t)=0$の根を</description>
    </item>
    
    <item>
      <title>PyTorchでCNN入門</title>
      <link>https://mosko.tokyo/post/pytorch_cnn/</link>
      <pubDate>Sat, 10 Jun 2017 15:37:12 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/pytorch_cnn/</guid>
      <description>CNNの概説 CNNは畳み込みニューラルネットワーク(convolutional neural network)の略です．CNNは四天王のひとりLeCun(1989)に始まり，2012年の一般物体認識のコンテスト(ILSVRC)で優勝しディープラーニングを一躍有名にしたAlexNet(Krizhevsky)を経て，現在の画像認識には欠かせないネットワークです．
畳み込み CNNでは畳み込み(convolution)という操作を行います．ここでは簡単のためにすべて2次元で考えます．
以下のように入力の行列とフィルタが与えられたときに，
\[I=ar+bs+tc+du+ev+fw+gz+hy+iz\]
を畳み込みと呼びます．本来画像認識の分野では$ax+by+cz+\cdots$を畳み込みと呼び，上記の演算は相関と呼ばれるようですが，CNNの文脈ではこれを畳み込みと呼ぶよう1なので慣例に倣います．
 左が入力の一部，右がフィルター．   入力に対して，この操作を同じフィルタをずらしながら適用していきます．下の図では上部の入力とフィルタの畳み込み結果を下の出力行列の各要素にする様子を書きました．こうして畳み込みによる出力が得られます．
 上が入力とフィルタ，下が出力．   畳み込みは画像の対応部分とフィルタとの内積を取ることですから，それらの関連ぐらいを見ていることになります（それ故に相関と呼ばれるのですが）．従って，出力は入力画像のフィルタとの関連度を凝縮したものになるわけです．以上の畳み込み（あるいは相関）自体はCNN以前から画像認識の分野で用いられてきましたが，CNNではフィルタ自体を誤差逆伝播法で学習していく点が従来とは異なります2．
上では入力，フィルタとも1枚ずつである場合を考えましたが，一般にそれらは複数枚あり，テンソルとして扱われます．この「枚数方向」の次元はチャネルと呼ばれます．特にRGB画像は3チャネルです．
複数チャネルの場合は，入力の各チャネルに対して同一のフィルタを適用し，その和をとります．従って，出力のチャネル数はフィルタ数と一致します．
プーリング CNNではその他にプーリングという操作を行う場合もあります．その中でもよく用いられる最大プーリング(max pooling)は下に示したように，領域内の最大値を取り出して出力とする操作です．画像認識では位置がずれた同じ物体も同じものとして認識したいので，この操作を加えて位置に対する不変性を向上させます．
 最大プーリング．上部が入力で下部が出力．   最大プーリングのほかに，平均値を用いるプーリングもあります．
用語 説明に用いる画像はこちらのもので，今までと異なり下が入力，上が出力です．
kernel 上記の畳み込みのフィルタやプーリングの領域のことをカーネルと呼ぶこともあります．
stride カーネルの動く際のステップです．プーリングの場合は領域幅と同じ幅で動かし，重複する範囲がないようにすることが多い気がします．
 stride=1   padding 畳み込み，プーリングを上記のように行った場合，出力は入力よりも小さくなります．入力の周りに「枠」を付けることで出力サイズを調整するのがpaddingです．「枠」を0で埋めるゼロパディングがしばしば用いられます．
 stride=1,padding=1   dilation カーネルにあける隙間の大きさです．プーリングの代わりにdilationを用いることもあるようです．
 stride=1,dilation=1   relu 活性化函数の一つで，“rectified linear unit”の略です．函数としては
\[\mathrm{relu}(x)=\max(0,x)\]
と極めて単純ですが，これがなければ現在のディープニューラルネットワーク時代はなかった，とも言えるような，強力な存在です．以前はsigmoid函数(S字状函数)，たとえば
\[\mathrm{sigmoid}(x)=\frac{1}{1+e^{-x}}\]
が用いられていましたが，ネットワークが深くなると勾配が消失する問題を抱えていました．
 sigmoid函数とrelu函数との比較   PyTorchにおけるCNN PyTorchの簡単なチュートリアルはこちらにあります．
コード中のFはnn.functionalのことです．
nn nn.Conv2dを用います．F.conv2dというものもありますが，こちらは自分で明示的にweight,biasのテンソルを用意し，必要であれば重みを更新しなくてはいけません．他方，nn.Conv2dであれば入出力のチャネル数およびカーネルの大きさを定めるだけです．今回は用いていませんが，上で説明したpadding,dilationを用いることもできます．
プーリングには，畳み込みのように更新すべきテンソルがないのでF.max_pool2dを用いても差はありません．</description>
    </item>
    
    <item>
      <title>PyTorchでディープラーニング</title>
      <link>https://mosko.tokyo/post/pytorch_tutorial/</link>
      <pubDate>Thu, 08 Jun 2017 16:33:50 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/pytorch_tutorial/</guid>
      <description>PyTorchとは PyTorchはFacebookの開発するPython上でのテンソル計算・自動微分ライブラリで，特にディープラーニングに必要な機能が充実しています．2017年の初頭に公開され，瞬く間にTensorflow, Keras, Caffeに続くディープラーニングライブラリとして人気を博すこととなりました．
Bonus: stars (not an indicator of usage, just proportional to how many people have landed on the GitHub page over the period). pic.twitter.com/IugHJqHSii
&amp;mdash; François Chollet (@fchollet) April 12, 2017  PyTorchはPreferred NetworkのディープラーニングライブラリChainerから影響を受けており，GoogleのTensorFlowやUniversité de MontréalのTheanoとは異なり，実行時に動的にグラフを構築するため，柔軟なコードを書くことができます．
PyTorchは，製品にも用いられているTensorFlowとは異なり，研究向けであることが明言されています．新機能の変更は多いものの，疎テンソルにいち早く対応するなど，最新の研究動向を追うにはよいのではないでしょうか．また，適当なレベルで書くことができて，素のTensorflowのように低レベルでもなく，Kerasの様に高度に抽象化されているわけでもなく，ラッパーによって書き方が多様でサンプルを見てもよく分からない，ということはないので，学びやすいと思います．
チュートリアル とりあえず動かせるようになるチュートリアルです．
インストール GPU環境は勿論，CPU環境でも動かすことができます．Linux，macOSの場合は 公式, Windowsの場合は Anaconda Cloudからインストールできます．
GPUを利用する場合，環境の設定が面倒なことが多いですがPyTorchでは特に設定せずにGPU対応版をダウンロードするとGPUが使えるようになるようです．
Tensor PyTorchの基本はテンソルを操作するTensorです．テンソルというと難しく聞こえますが，この場合は多次元配列と同義で，物理学のテンソルのような共変・反変を意識する必要はありません．慣例に従ってテンソル，と言う語を用います．
PyTorchにおけるTensorは端的に言えば「GPU上でも動くnumpy.ndarrayのようなもの」ですが，違いも多いので注意が必要です．例えば
import numpy as np import torch # PyTorch &amp;gt;&amp;gt;&amp;gt; np_tensor = np.zeros([1, 2, 3]) array([[[ 0., 0., 0.</description>
    </item>
    
    <item>
      <title>PyTorchはじめ</title>
      <link>https://mosko.tokyo/post/getting_started_pytorch/</link>
      <pubDate>Tue, 24 Jan 2017 15:15:55 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/getting_started_pytorch/</guid>
      <description>先日Facebookが PyTorch を公開していたので，早速試してみた．PyTorchは
 Tensors and Dynamic neural networks in Python with strong GPU acceleration.
 とのことで，TensorFlowやTheanoより，Chainerに似ている気がする．後発ということもあってか，ウェブページにある導入の説明が丁寧で，Linux，Python 3.5，conda，Cuda8.0なら
conda install pytorch torchvision cuda80 -c soumith  を叩くだけでよい．その下にはMNISTなどの例やJupyterのチュートリアルへのリンクがあるのも丁寧．ただ，ニューラルネットワークの知識に乏しくともレイヤーを重ねてscikit-learn風によしなにすればよいkerasよりは難しいが，その分柔軟に書けそう．メモリを大量消費するTensorFlowに較べて，GPUに対する負荷はかなり小さそう．
基本的にはnn.Moduleを継承してネットワークを定義する． 以下のコードはGithubに挙げた．
class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5, stride=1) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) self.conv2_bn = nn.BatchNorm2d(20) self.dense1 = nn.Linear(in_features=320, out_features=50) self.dense1_bn = nn.BatchNorm1d(50) self.dense2 = nn.Linear(50, 10) def forward(self, x): x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = F.relu(F.max_pool2d(self.conv2_bn(self.conv2(x)), 2)) x = x.</description>
    </item>
    
    <item>
      <title>「日本古典籍字形データセット」で遊ぶ</title>
      <link>https://mosko.tokyo/post/mnist_kuzushiji/</link>
      <pubDate>Fri, 13 Jan 2017 13:18:40 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/mnist_kuzushiji/</guid>
      <description>日本語版MNIST,というわけではないけれど日本古典籍字形データセットの識別をkerasで実装したresnetによって行った．現在validation accuracyは93.3%．少なくとも自分よりはきちんと分類できるようだ．
このデータセットには2017年1月現在，「8点の画像データから切り取ったくずし字1,521文字種の字形データ86,176文字」が収録されているので，そのまま1521に分類している．
今回はMNIST的に使うので，つまり文脈を考慮しないので変体仮名の「志」（し）と漢字としての「志」とを区別する，というようなタスクも含まれてしまうが，特に考慮しない．kerasのImageDataGeneratorで前処理を一括して行う．本当はもう少し丁寧にした方がいいのかもしれないけれど，とりあえず．
 # data generator train_datagen = ImageDataGenerator( shear_range=0.05, width_shift_range=0.05, height_shift_range=0.05, rotation_range=10, fill_mode=&amp;quot;constant&amp;quot;, cval=200, zoom_range=0.2) train_generator = train_datagen.flow_from_directory( &#39;train&#39;, color_mode=&amp;quot;grayscale&amp;quot;, target_size=target_size, batch_size=batch_size, class_mode=&#39;categorical&#39; ) val_datagen = ImageDataGenerator() val_generator = val_datagen.flow_from_directory( &#39;val&#39;, color_mode=&amp;quot;grayscale&amp;quot;, target_size=target_size, batch_size=batch_size, class_mode=&#39;categorical&#39; )  training dataには変形を施した．resnetはkeras.jsを参考にして実装(下記のres_a,res_b)．
# model input_layer = Input(shape=input_shape) x = Convolution2D(nb_filters, 4, 4, subsample=(2,2))(input_layer) x = BatchNormalization()(x) x = Activation(&#39;relu&#39;)(x) x = MaxPooling2D(pool_size, strides=stride_size)(x) x = res_a([32,32,128])(x) x = res_b([32,32,128])(x) x = res_b([32,32,128])(x) x = res_a([64,64,256])(x) x = res_b([64,64,256])(x) x = res_b([128,128,256])(x) x = res_a([128,128,512])(x) x = res_b([128,128,512])(x) x = res_b([256,256,512])(x) x = AveragePooling2D((4,4))(x) x = Flatten()(x) output_layer = Dense(nb_classes, activation=&#39;softmax&#39;)(x) model = Model(input=input_layer, output=output_layer) model.</description>
    </item>
    
    <item>
      <title>dotfilesを公開</title>
      <link>https://mosko.tokyo/post/dotfiles/</link>
      <pubDate>Sat, 12 Nov 2016 01:11:08 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/dotfiles/</guid>
      <description>研究室の人々もすなるdotfilesなるものを，我も公開してみんとてするなり．それの年のかむなつきの廿日あまり一日の，戌の時に部屋の人のいふやう，「dotfilesを見せよ」．
公開して，人のdotfilesを見るようになると段々充実してきて諸々使いやすくなった一方忘れやすくもなったので備忘録をば．
Vim(NeoVim) 新しいもの好きなのでNeoVim ，プラグインマネージャーとしてdein.nvimを使っている．
denite.nvim dein.nvimの設定ファイルに
[[plugins]] repo = &#39;Shougo/denite.nvim&#39;  を追加する．基本的にここを参考にしてマッピングした．&amp;lt;C-u&amp;gt;&amp;lt;C-g&amp;gt;でgrepによるファイルの検索のようなことが高速に行える．
vim-easy-align [[plugins]] repo = &#39;junegunn/vim-easy-align&#39;  Githubを見れば一目瞭然なのだが，gaで起動するように設定しておくだけで，vipga=のみで上から下を実現する．
apple = red sky = blue banana=yellow --------------- apple = red sky = blue banana = yellow  ともかく上記のREADMEが非常に充実しているのでこれを見る．
misc  &amp;lt;C-v&amp;gt;でヴィジュアルモードに入り，範囲選択してI#で選択した行のコメントアウト．  tmux どうして今まで知らなかったのだろう，tmux．
さまざまな機能があるけれども，サーバとのsshの接続を切ってしまってもプロセスを動かし続けることが出来る，というのがいちばんありがたい．それまでは時間のかかるプロセスを動かしてしまった日はノートパソコンが据え置き機と化してしまっていた．
tmux aで前回のセッションにつないで，tmux上でCtrl+dで離れる．プレフィックスキーはzshのCtrl+aを多用するので標準のCtrl+bのままにしてある．マウス/タッチパッドの使用を有効にする設定がバージョンによって違うのでそれが多少厄介で，今回のdotfilesではtmuxディレクトリを作っておき，バージョンによって読み込むファイルを変えることで解決している．</description>
    </item>
    
    <item>
      <title>Juliaに触ってみた</title>
      <link>https://mosko.tokyo/post/julia-boxmuller/</link>
      <pubDate>Sun, 23 Oct 2016 22:36:34 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/julia-boxmuller/</guid>
      <description>我らがJupyterのJu,であるところのJupyterに触って，IPythonに相当するIJuliaを導入してJupyterから操作してみた．ちょっと触った感想は，強いR-lang．
IJuliaの導入． Juliaは ここから導入する．ターミナルから開いて，
 Package.add(&amp;quot;IJulia&amp;quot;) using IJulia notebook()  これでJupyterが起動する．あとは普段通り．
触る． 折角なので手元にあったPRMLにあった，一様乱数からガウス分布を得るBox-Muller法によって得られる分布をプロットする．以下ではプロットツールのGadflyを用いている．
using Gadfly set_default_plot_size(10cm,10cm); # Box-Muller法 function box_muller(num) x = [] y = [] for i = 0:num a = rand() b = rand() η = 2a -1 #1 ζ = 2b -1 r2 = η ^ 2 + ζ ^ 2 if (r2) &amp;lt;= 1 x = push!(x, η * √(-2 * log(r2) / r2)) #2 y = push!</description>
    </item>
    
    <item>
      <title>PythonでMySQLを使う</title>
      <link>https://mosko.tokyo/post/python-and-mysql/</link>
      <pubDate>Sat, 22 Oct 2016 00:21:34 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/python-and-mysql/</guid>
      <description>現在開発しているものはScalaで前処理を行っているので，本番の処理も本当は全部Scalaで書ければよかったのだけれども，そうは問屋が卸さず，種々の原因によりPythonが必要になってしまった． これなら最初から全部Pythonでよかったのでは，とも思うけれど，PythonはJupyterで小さなものを色々弄るのには使うものの，大きいものをPythonで書いた経験が無いので心配．型が違う，という注意が沢山出そうだ．
ともかく，そのためにPythonからMySQLを扱う必要が出てきた．Pythonists3は新しいもの好きなのか，いまやNoSQLを使うのがトレンドなのか，MySQL周りの情報が少ないのだが，pymysqlを使うことに落ち着いた．
connection = pymysql.connect(host=&amp;quot;HOSTNAME&amp;quot;, user=&amp;quot;USERNAME&amp;quot;, password=&amp;quot;PASSWORD&amp;quot;, db=&amp;quot;DB_NAME&amp;quot;, charset=&#39;utf8&#39;, cursorclass=pymysql.cursors.DictCursor) #1 with connection.cursor() as cursor: sql = &amp;quot;SELECT name FROM table WHERE id=%s&amp;quot; cursor.execute(sql, (900)) results = cursor.fetchall() for r in results: b = r[&#39;name&#39;] print(bytes.decode(b)) #2  #1を指定することで，返ってくる結果がdict形式になって分かりやすい．
最後，#2でnameに相当する列がvarbinaryであったので，文字列に変換するのにbytes.decode()が必要だった．とりあえずこれで一件落着．</description>
    </item>
    
    <item>
      <title>MeCabのJavaバインドをIntelliJで使う．</title>
      <link>https://mosko.tokyo/post/use-mecab-java-with-intellij/</link>
      <pubDate>Sat, 08 Oct 2016 19:57:29 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/use-mecab-java-with-intellij/</guid>
      <description>MeCabのJavaバインドのセットアップ MeCab公式からJavaバインドをダウンロードし，解凍する．Linuxであればこのままmakeなのだが，macOSであれば， MeCab のJava バインディングをMacOSX10.8.3(Mountain Lion) でScala から使うを参考に，Makefileを書き換えてmake．
 xcode-select --installを実行し，xcodeのコマンドラインツールを導入する．
 Makefileのスペースをタブで置き換える．
  必要があった．ここで
javac org/chasen/mecab/*.java  などを試す．エラーが出なければ，インストールは出来ているはず．
IntelliJで使う． stackoverflowにあった通りなのだが，run/debug congfigurationのJava
 VM option: -Djava.library.path=&amp;quot;/usr/local/bin/mecab-java&amp;quot;  とする必要があった．これがないと，以下のエラーが生じる．
 Exception in thread &amp;quot;main&amp;quot; java.lang.UnsatisfiedLinkError: no MeCab in java.library.path at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1867) at java.lang.Runtime.loadLibrary0(Runtime.java:870) at java.lang.System.loadLibrary(System.java:1122) at Mecab_test$.main(Mecab_test.scala:11) at Mecab_test.main(Mecab_test.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)  はじめ，java.library.pathというのは環境変数で設定するものと思って苦労した</description>
    </item>
    
    <item>
      <title>JUMAN&#43;&#43; on Ubuntu</title>
      <link>https://mosko.tokyo/post/juman-install/</link>
      <pubDate>Wed, 05 Oct 2016 14:40:43 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/juman-install/</guid>
      <description>日本語形態素解析には MeCabを使ってきたが，京大の JUMAN++が進化してWikipediaやWiktionaryの用語を取り込んでMeCabを追い越した，とのことなのでインストールしてみた．
インストール  必須ライブラリ
 gcc (4.9+)
 Boost C++ Libraries (1.57+)
  推奨ライブラリ
 gpreftool
 libunwind(gpreftoolを64bit環境で使用する場合)
   gccと推奨ライブラリはapt-get installで導入できたのだが，Boostはapt-getでは1.57以上が入らなかったので，boost.orgから最新版を解凍して
cd boost_* sudo ./bootstrap.sh --prefix=&amp;lt;INSTALL_PATH&amp;gt; sudo ./b2 install export PATH=$PATH:&amp;lt;INSTALL_PATH&amp;gt;/include/:&amp;lt;INSTALL_PATH&amp;gt;/lib/ export BOOST_ROOT=&amp;lt;INSTALL_PATH&amp;gt;  続いて，JUMAN++をダウンロードし，解凍する．
cd jumanapp-1.0* ./configure --with-boost=&amp;lt;INSTALL_PATH&amp;gt; sudo make install  こうして，jumanappが使えるようになる．
比較 JUMAN++
 入り口から入って振り返ると，善き羊飼いとしてのキリストの図像がある． 入り口 いりぐち 入り口 名詞 6 普通名詞 1 * 0 * 0 &amp;quot;代表表記:入り口/いりぐち カテゴリ:場所-その他&amp;quot; から から から 助詞 9 格助詞 1 * 0 * 0 NIL 入って はいって 入る 動詞 2 * 0 子音動詞ラ行 10 タ系連用テ形 14 &amp;quot;代表表記:入る/はいる 自他動詞:他:入れる/いれる 反義:動詞:出る/でる&amp;quot; 振り返る ふりかえる 振り返る 動詞 2 * 0 子音動詞ラ行 10 基本形 2 &amp;quot;代表表記:振り返る/ふりかえる&amp;quot; と と と 助詞 9 格助詞 1 * 0 * 0 NIL ， ， ， 特殊 1 読点 2 * 0 * 0 NIL 善き よき 善い 形容詞 3 * 0 イ形容詞アウオ段 18 文語連体形 21 &amp;quot;代表表記:良い/よい 反義:形容詞:悪い/わるい&amp;quot; @ 善き よき 善い 形容詞 3 * 0 イ形容詞アウオ段 18 文語連体形 21 &amp;quot;代表表記:良い/よい 反義:形容詞:悪い/わるい&amp;quot; 羊飼い ひつじかい 羊飼い 名詞 6 普通名詞 1 * 0 * 0 &amp;quot;代表表記:羊飼い/ひつじかい カテゴリ:人&amp;quot; と と と 助詞 9 格助詞 1 * 0 * 0 NIL して して する 動詞 2 * 0 サ変動詞 16 タ系連用テ形 14 &amp;quot;代表表記:する/する 付属動詞候補（基本） 自他動詞:自:成る/なる&amp;quot; の の の 助詞 9 接続助詞 3 * 0 * 0 NIL キリスト キリスト キリスト 名詞 6 普通名詞 1 * 0 * 0 &amp;quot;自動獲得:Wikipedia Wikipedia多義&amp;quot; の の の 助詞 9 接続助詞 3 * 0 * 0 NIL 図像 ずぞう 図像 名詞 6 普通名詞 1 * 0 * 0 &amp;quot;代表表記:図像/ずぞう カテゴリ:抽象物 ドメイン:文化・芸術&amp;quot; が が が 助詞 9 格助詞 1 * 0 * 0 NIL ある ある ある 動詞 2 * 0 子音動詞ラ行 10 基本形 2 &amp;quot;代表表記:有る/ある 補文ト 反義:形容詞:無い/ない&amp;quot; ． ． ． 特殊 1 句点 1 * 0 * 0 NIL EOS  MeCab</description>
    </item>
    
    <item>
      <title>Jupyterをサーバー上で使う</title>
      <link>https://mosko.tokyo/post/using-jupyter-on-an-external-server/</link>
      <pubDate>Wed, 31 Aug 2016 18:30:08 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/using-jupyter-on-an-external-server/</guid>
      <description>2016-08-29 サーバー上のJupyter notebookを使う サーバー上でPythonを実行するのに，ターミナルで弄っていたが何かと不便だったのでJupyterを導入した．
jupyter notebook --generate-config vim ~/.jupyter/jupyter_notebook_config.py  でjupyter_notebook_config.pyに以下を加える．
 c.NotebookApp.ip = &#39;*&#39; # localhost以外からもアクセス可能にする。 c.NotebookApp.port = 9999 # サーバのポートを指定。デフォルト8888。 c.NotebookApp.open_browser = False # ブラウザが自動で開かないようにする。 c.NotebookApp.notebook_dir = &#39;/home/USER_NAME/notebooks&#39; # 作業ディレクトリを指定。デフォルト起動ディレクトリ。  かくして，jupyter notebookコマンドを叩くとhttp://hoge.hoge:9999でJupyterが扱える．
scikit-learnでのMKLエラー解決 Ubuntuサーバー上のAnaconda3でscikit-learnを動かしたところ
Intel MKL FATAL ERROR: Cannot load libmkl_avx.so or libmkl_def.so  というような表示が出て終了してしまう．とりあえず
conda install nomkl numpy scipy scikit-learn numexpr  で解決させたものの，MKL使った方が速そうなので早々に解決したい．MROのMKLが悪かったりするのだろうか．
参考
Jupyter Notebook（IPython）サーバの起動方法</description>
    </item>
    
  </channel>
</rss>