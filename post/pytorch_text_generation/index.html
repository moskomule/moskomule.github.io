
<!DOCTYPE html>
<html lang="ja">
<head>

  
  <meta charset="UTF-8">
  <title>
    PyTorchでテキスト生成 | moskomule log
  </title>


  
  <meta name="viewport" content="width=device-width,user-scalable=no,maximum-scale=1,initial-scale=1">

  
  <link rel="canonical" href="http://mosko.tokyo/post/pytorch_text_generation/"/>

  
  <link rel="stylesheet" href="/css/sanitize.css">
  <link rel="stylesheet" href="/css/responsive.css">
  <link rel="stylesheet" href="/css/highlight_monokai.css">
  <link rel="stylesheet" href="/css/theme.css">
  <link rel="stylesheet" href="/css/custom.css">

  
  <link href="http://mosko.tokyo/index.xml" rel="alternate" type="application/rss+xml" title="moskomule log" />
  <link href="http://mosko.tokyo/index.xml" rel="feed" type="application/rss+xml" title="moskomule log" />

  
  

  
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  },
  CommonHTML: { matchFontHeight: false }
});
</script>
<script async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
</script>
</head>



<body>
<div class="container">

  
  <header role="banner">
    <div class="row gutters">
      <div id="site-title" class="col span_6">
        <h1><a href="http://mosko.tokyo/">moskomule log</a></h1>
        <h2>carpe diem</h2>
      </div>
      <div id="social" class="col span_6">
        <ul>
          <li><a href="mosko_mule" target="_blank">Twitter</a></li>
          
          <li><a href="moskomule" target="_blank">GitHub</a></li>
          <li><a href="http://mosko.tokyo/index.xml" type="application/rss+xml" target="_blank">RSS</a></li>
        </ul>
      </div>
    </div>
  </header>


  
  <main id="single" role="main">
    <div class="article-header">
      <h1>PyTorchでテキスト生成</h1>
      <div class="meta">
        Jan 28, 2017 &nbsp;
        
          #<a href="/tags/deep-learning">Deep Learning</a>&nbsp;
        
          #<a href="/tags/pytorch">PyTorch</a>&nbsp;
        
      </div>
    </div>
    <article>
      <p>相変わらずPyTorchをいじっている．後発のこともあって，まだDocは完全ではないけれど，Discussionなどのサポート体制は充実している(気がする)．</p>

<p>コミュニティの助けを借りてRNNでの<a href="https://github.com/moskomule/PyTorch_learn/blob/master/simple/rnn_text_gen.py">テキスト生成</a>をおこなった．これはKerasの<a href="https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py">サンプル</a>をPyTorchで書き換えたもので時流に乗ってオーウェルの <em>1984</em> を学習する．Kerasのように内部状態を特に考える必要がないのとは異なって，隠れ変数$h_{\star}$を意識しなくてはいけないので勉強になる．</p>

<ul>
<li><p>function <code>var</code>: CUDAが使えれば<code>torch.autograd.Variable</code>をGPUにおく(<code>variable.cuda()</code>)．</p></li>

<li><p>function <code>sample</code>: RNNモデルの出力から適当なindexを取り出す．</p></li>

<li><p>function <code>__init__</code> in class <code>Net</code>: <code>input</code>はfeature数で，今回であればアルファベットをonehotにしているので，<code>len(chars)</code>．</p></li>
</ul>

<pre><code>def __init__(self, features, cls_size):
    super(Net, self).__init__()
    self.rnn1 = nn.GRU(input_size=features,
                        hidden_size=hidden_size,
                        num_layers=1)
    self.dense1 = nn.Linear(hidden_size, cls_size)
</code></pre>

<ul>
<li>function <code>forward</code> in class <code>Net</code>: 系列の最後の入力に対する隠れ層の状態をとるために<code>x=select(0, maxlen-1)</code>を行っている(追記:実は<code>x[-1]</code>で充分)．<code>reshape</code>に相当する<code>view</code>を行うためにはcontiguousが必要．またテンソル<code>x</code>は$\text{系列の長さ}\times\text{バッチ数}\times\text{feature数}$である点に注意．</li>
</ul>

<pre><code>def forward(self, x, hidden):
    x, hidden = self.rnn1(x, hidden)
    x = x.select(0, maxlen-1).contiguous()
    x = x.view(-1, hidden_size)
    x = F.softmax(self.dense1(x))
    return x, hidden
</code></pre>

<ul>
<li>function <code>train</code>: 入力した文を1通り読み込むのを1エポックにしている．Kerasと異なり，PyTorchの<code>CrossEntropyLoss</code>では<code>target</code>はクラスのインデックスである．この目標のインデクス配列の型は<code>np.array(dtype=np.int)</code>だが，これは<code>torch.Tensor</code>では<code>Tensor</code>に変換できないので<code>torch.LongTensor</code>を用いている．</li>
</ul>

<pre><code>def train():
    model.train()
    hidden = model.init_hidden()
    for epoch in range(len(sentences) // batch_size):
        X_batch = var(torch.FloatTensor(X[:, epoch*batch_size: (epoch+1)*batch_size, :]))
        y_batch = var(torch.LongTensor(y[epoch*batch_size: (epoch+1)*batch_size]))
        model.zero_grad()
        output, hidden = model(X_batch, var(hidden.data))
        loss = criterion(output, y_batch)
        loss.backward()
        optimizer.step()
    print(&quot;\r{}&quot;.format(loss.data[0]), end=&quot;&quot;)
</code></pre>

<p>主要な部分は以上．特に<code>Variable</code>に<code>Variable</code>を与えるとエラーになったり，いちいち<code>Variable().cuda()</code>をしなくてはいけないところで躓いたりした．また，<code>Tensor</code>は暗黙的に<code>FloatTensor</code>に<a href="http://PyTorch.org/docs/tensors.html">変換される</a>のもポイント．</p>

      
      
      
    </article>
    


  </main>
  
  <nav class="pagination-single">
    
      <span class="previous">&larr; <a href="http://mosko.tokyo/post/getting_started_pytorch/" rel="prev">PyTorchはじめ</a></span>
    
    
  </nav>


  
  <footer role="contentinfo">
    <div style="text-align:center;">
      
      Written by Ryuichiro Hataya
    </div>
  </footer>


</div>

<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



</body>
</html>

