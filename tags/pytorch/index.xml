<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pytorch on moskomule log</title>
    <link>https://mosko.tokyo/tags/pytorch/</link>
    <description>Recent content in Pytorch on moskomule log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <copyright>Written by Ryuichiro Hataya</copyright>
    <lastBuildDate>Sun, 01 Oct 2017 19:40:24 +0900</lastBuildDate>
    
	<atom:link href="https://mosko.tokyo/tags/pytorch/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Double Backpropagationについて</title>
      <link>https://mosko.tokyo/post/double-backprop/</link>
      <pubDate>Sun, 01 Oct 2017 19:40:24 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/double-backprop/</guid>
      <description>はじめに PyTorch v0.2では&amp;rdquo;Higher order gradients&amp;rdquo; (double backpropagation)がサポートされました．Chainerもv3においてこれがサポートされます．今回Chainer Meetupの資料を読んで雰囲気が分かったのでまとめました．
 Comparison of deep learning frameworks from a viewpoint of double backpropagation  Chainer v3  筆者は長くdouble backpropagationという名称から
\[\mathrm{loss}\longrightarrow \frac{\partial^2 \mathrm{loss}}{\partial x_i \partial x_j} \]
と思い込んでいました．そう思っているのでdocumentを読んでもいまいちよく分からない．ところが上に挙げた資料では，そうではなくて
\[\mathrm{loss}=g(f(x), \frac{\partial f(x)}{\partial x})\]
のようなことなのだ，ということが説明されていて救われました．
PyTorchの例 これで以上，でもよいのですが，PyTorchでの例を．
$x=1, y=x^3, z=y^2+\frac{dy}{dx}$をとして，$\frac{dz}{dx}|_{x=1}$を求めます．
&amp;gt;&amp;gt;&amp;gt; x = Variable(torch.Tensor([1]), requires_grad=True) &amp;gt;&amp;gt;&amp;gt; y = x ** 3 &amp;gt;&amp;gt;&amp;gt; grad_y, = autograd.grad(y, x, create_graph=True) &amp;gt;&amp;gt;&amp;gt; (grad_y + y ** 2).backward() &amp;gt;&amp;gt;&amp;gt; x.grad Variable containing: 12 [torch.</description>
    </item>
    
    <item>
      <title>PyTorchでRNN入門</title>
      <link>https://mosko.tokyo/post/pytorch_rnn/</link>
      <pubDate>Sat, 24 Jun 2017 15:37:12 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/pytorch_rnn/</guid>
      <description>RNNの概説 RNNは再帰型ニューラルネットワーク(recurrent neural network)の略です．各層は以前の自分自身の出力も入力とする再帰的な構造をもつため，この名がつけられています．時間依存のある文や信号といった入力を処理することができます．
 RNN（左）とCNNなどのネットワーク（右）．RNNは自分自身の出力も入力として取り込むことで，時間に依存した情報を扱うことができると考えられた．   RNN自体は90年代初頭にJ.Elmanらによって提案され1，文生成や分散表現の獲得などの研究が行われています．現在でもよく使われる，より長い系列にも対応できるLSTMも90年代末に提案されており，伝統のあるネットワークであるといえるでしょう．
画像におけるCNNの華々しい活躍と比較すると劣りますが，それでもGoogle翻訳の昨今の「自然な」翻訳の背景にはRNNがあります．
単純なRNN 入力とする系列$x_0,x_1,\cdots,x_T$を$x_0$から順次与えていきます．ここでは下付き文字$\star_t$は時刻を表します．また上付き文字$\star^l$を$l$層目の状態として，時刻$t$における$l$層目の隠れ状態を$h_t^l$，出力を$y_t$と表します．
再帰型ではないニューラルネットワークでは，ある層$l$の隠れ状態$h^l$は，その前の層への状態に重み$W^l$をかけ，活性化函数$f$に与えたもので，
\[h^l = f(W^lh^{l-1})\]
でした（ただし簡便のためにバイアスは省きました．今後も同様です．）．
一方で，RNNには時刻の概念があり，さらに一つ前の状態を考慮するため，ある時刻$t$における，層$l$の状態$h^l_t$は
\[h^l_t = f(W^lh_{t}^{l-1}+U^lh_{t-1}^l)\]
です．つまり，前の層の出力$W^lh_t^{l-1}$に，前時刻の自分の出力$U^lh_{t-1}^l$が加わったものを活性化函数に与えることとなります．活性化函数$f$としては$\tanh,\mathrm{relu},\mathrm{sigmoid}$などが用いられます．
それでは実際に系列を入力してみましょう．まず，$x_0$を入力します．
 時刻 $\sim 0$   このとき$t=0$では，上の式から
\[h^1_0=f(W^1x_0+U^1h_{-1}^1),h^2_0=f(W^2h^1_0+U^2h_{-1}^2)\]
となります．この$h_{-1}^1,h_{-1}^2$は最初は隠れ状態がないために与える必要がある「仮の隠れ状態」で，$0$など適当に初期化されたベクトルを用います．同様にして，recurrent層が$L$層あれば時刻0において$x_0$と$h_{-1}^1,h_{-1}^2,\cdots,h_{-1}^L$を用意する必要があります．また，最終層は
\[y_t=f_y(W^{L}h_t^{L})\]
で与えられます．
その後は隠れ状態があるので，順次
\[h^1_1=f(W^1x_1+U^lh_{0}^1)\]
などとなります．
  時刻 $0\sim 1$    時刻 $1\sim 2$    時刻 $2\sim $   重みの更新は一つの系列が終了してから行います．このとき用いる損失は，目標を$d_0,d_1,\cdots,d_T$として，すべての時刻に対して出力が必要な場合，例えば文章生成の場合，
\[\sum_t\mathrm{loss}(y_t,d_t)\]
とします．または，二値分類などでは$y_T$には$y_0,\cdots,y_{T-1}$の情報が蓄積されていると考えて
\[\mathrm{loss}(y_T, d_T)\]
を用います．
いずれにしても，このとき$t=T$での損失から$t=0$での隠れ状態も考慮することとなります．上の図では$t=2$までしかありませんが，$h_0^1$から$y_2$までの経路は，例えば$h_0^1\to h_1^1\to h_1^2\to h_2^2\to y_2$などのように，一般のネットワークでは隠れ層4のネットワークに相当します．
そのため，理論的には長い系列を処理することができますが，実際にはこのような単純なRNNでは容易に勾配消失がおこり，長い系列は学習できなくなることが知られています．
PyTorchではこの単純なRNNはnn.RNNに用意されています（後述）．</description>
    </item>
    
    <item>
      <title>PyTorchでCNN入門</title>
      <link>https://mosko.tokyo/post/pytorch_cnn/</link>
      <pubDate>Sat, 10 Jun 2017 15:37:12 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/pytorch_cnn/</guid>
      <description>CNNの概説 CNNは畳み込みニューラルネットワーク(convolutional neural network)の略です．CNNは四天王のひとりLeCun(1989)に始まり，2012年の一般物体認識のコンテスト(ILSVRC)で優勝しディープラーニングを一躍有名にしたAlexNet(Krizhevsky)を経て，現在の画像認識には欠かせないネットワークです．
畳み込み CNNでは畳み込み(convolution)という操作を行います．ここでは簡単のためにすべて2次元で考えます．
以下のように入力の行列とフィルタが与えられたときに，
\[I=ar+bs+tc+du+ev+fw+gz+hy+iz\]
を畳み込みと呼びます．本来画像認識の分野では$ax+by+cz+\cdots$を畳み込みと呼び，上記の演算は相関と呼ばれるようですが，CNNの文脈ではこれを畳み込みと呼ぶよう1なので慣例に倣います．
 左が入力の一部，右がフィルター．   入力に対して，この操作を同じフィルタをずらしながら適用していきます．下の図では上部の入力とフィルタの畳み込み結果を下の出力行列の各要素にする様子を書きました．こうして畳み込みによる出力が得られます．
 上が入力とフィルタ，下が出力．   畳み込みは画像の対応部分とフィルタとの内積を取ることですから，それらの関連ぐらいを見ていることになります（それ故に相関と呼ばれるのですが）．従って，出力は入力画像のフィルタとの関連度を凝縮したものになるわけです．以上の畳み込み（あるいは相関）自体はCNN以前から画像認識の分野で用いられてきましたが，CNNではフィルタ自体を誤差逆伝播法で学習していく点が従来とは異なります2．
上では入力，フィルタとも1枚ずつである場合を考えましたが，一般にそれらは複数枚あり，テンソルとして扱われます．この「枚数方向」の次元はチャネルと呼ばれます．特にRGB画像は3チャネルです．
複数チャネルの場合は，入力の各チャネルに対して同一のフィルタを適用し，その和をとります．従って，出力のチャネル数はフィルタ数と一致します．
プーリング CNNではその他にプーリングという操作を行う場合もあります．その中でもよく用いられる最大プーリング(max pooling)は下に示したように，領域内の最大値を取り出して出力とする操作です．画像認識では位置がずれた同じ物体も同じものとして認識したいので，この操作を加えて位置に対する不変性を向上させます．
 最大プーリング．上部が入力で下部が出力．   最大プーリングのほかに，平均値を用いるプーリングもあります．
用語 説明に用いる画像はこちらのもので，今までと異なり下が入力，上が出力です．
kernel 上記の畳み込みのフィルタやプーリングの領域のことをカーネルと呼ぶこともあります．
stride カーネルの動く際のステップです．プーリングの場合は領域幅と同じ幅で動かし，重複する範囲がないようにすることが多い気がします．
 stride=1   padding 畳み込み，プーリングを上記のように行った場合，出力は入力よりも小さくなります．入力の周りに「枠」を付けることで出力サイズを調整するのがpaddingです．「枠」を0で埋めるゼロパディングがしばしば用いられます．
 stride=1,padding=1   dilation カーネルにあける隙間の大きさです．プーリングの代わりにdilationを用いることもあるようです．
 stride=1,dilation=1   relu 活性化函数の一つで，“rectified linear unit”の略です．函数としては
\[\mathrm{relu}(x)=\max(0,x)\]
と極めて単純ですが，これがなければ現在のディープニューラルネットワーク時代はなかった，とも言えるような，強力な存在です．以前はsigmoid函数(S字状函数)，たとえば
\[\mathrm{sigmoid}(x)=\frac{1}{1+e^{-x}}\]
が用いられていましたが，ネットワークが深くなると勾配が消失する問題を抱えていました．
 sigmoid函数とrelu函数との比較   PyTorchにおけるCNN PyTorchの簡単なチュートリアルはこちらにあります．
コード中のFはnn.functionalのことです．
nn nn.Conv2dを用います．F.conv2dというものもありますが，こちらは自分で明示的にweight,biasのテンソルを用意し，必要であれば重みを更新しなくてはいけません．他方，nn.Conv2dであれば入出力のチャネル数およびカーネルの大きさを定めるだけです．今回は用いていませんが，上で説明したpadding,dilationを用いることもできます．
プーリングには，畳み込みのように更新すべきテンソルがないのでF.max_pool2dを用いても差はありません．</description>
    </item>
    
    <item>
      <title>PyTorchでディープラーニング</title>
      <link>https://mosko.tokyo/post/pytorch_tutorial/</link>
      <pubDate>Thu, 08 Jun 2017 16:33:50 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/pytorch_tutorial/</guid>
      <description>PyTorchとは PyTorchはFacebookの開発するPython上でのテンソル計算・自動微分ライブラリで，特にディープラーニングに必要な機能が充実しています．2017年の初頭に公開され，瞬く間にTensorflow, Keras, Caffeに続くディープラーニングライブラリとして人気を博すこととなりました．
Bonus: stars (not an indicator of usage, just proportional to how many people have landed on the GitHub page over the period). pic.twitter.com/IugHJqHSii
&amp;mdash; François Chollet (@fchollet) April 12, 2017  PyTorchはPreferred NetworkのディープラーニングライブラリChainerから影響を受けており，GoogleのTensorFlowやUniversité de MontréalのTheanoとは異なり，実行時に動的にグラフを構築するため，柔軟なコードを書くことができます．
PyTorchは，製品にも用いられているTensorFlowとは異なり，研究向けであることが明言されています．新機能の変更は多いものの，疎テンソルにいち早く対応するなど，最新の研究動向を追うにはよいのではないでしょうか．また，適当なレベルで書くことができて，素のTensorflowのように低レベルでもなく，Kerasの様に高度に抽象化されているわけでもなく，ラッパーによって書き方が多様でサンプルを見てもよく分からない，ということはないので，学びやすいと思います．
チュートリアル とりあえず動かせるようになるチュートリアルです．
インストール GPU環境は勿論，CPU環境でも動かすことができます．Linux，macOSの場合は 公式, Windowsの場合は Anaconda Cloudからインストールできます．
GPUを利用する場合，環境の設定が面倒なことが多いですがPyTorchでは特に設定せずにGPU対応版をダウンロードするとGPUが使えるようになるようです．
Tensor PyTorchの基本はテンソルを操作するTensorです．テンソルというと難しく聞こえますが，この場合は多次元配列と同義で，物理学のテンソルのような共変・反変を意識する必要はありません．慣例に従ってテンソル，と言う語を用います．
PyTorchにおけるTensorは端的に言えば「GPU上でも動くnumpy.ndarrayのようなもの」ですが，違いも多いので注意が必要です．例えば
import numpy as np import torch # PyTorch &amp;gt;&amp;gt;&amp;gt; np_tensor = np.zeros([1, 2, 3]) array([[[ 0., 0., 0.</description>
    </item>
    
    <item>
      <title>PyTorchはじめ</title>
      <link>https://mosko.tokyo/post/getting_started_pytorch/</link>
      <pubDate>Tue, 24 Jan 2017 15:15:55 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/getting_started_pytorch/</guid>
      <description>先日Facebookが PyTorch を公開していたので，早速試してみた．PyTorchは
 Tensors and Dynamic neural networks in Python with strong GPU acceleration.
 とのことで，TensorFlowやTheanoより，Chainerに似ている気がする．後発ということもあってか，ウェブページにある導入の説明が丁寧で，Linux，Python 3.5，conda，Cuda8.0なら
conda install pytorch torchvision cuda80 -c soumith  を叩くだけでよい．その下にはMNISTなどの例やJupyterのチュートリアルへのリンクがあるのも丁寧．ただ，ニューラルネットワークの知識に乏しくともレイヤーを重ねてscikit-learn風によしなにすればよいkerasよりは難しいが，その分柔軟に書けそう．メモリを大量消費するTensorFlowに較べて，GPUに対する負荷はかなり小さそう．
基本的にはnn.Moduleを継承してネットワークを定義する． 以下のコードはGithubに挙げた．
class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5, stride=1) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) self.conv2_bn = nn.BatchNorm2d(20) self.dense1 = nn.Linear(in_features=320, out_features=50) self.dense1_bn = nn.BatchNorm1d(50) self.dense2 = nn.Linear(50, 10) def forward(self, x): x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = F.relu(F.max_pool2d(self.conv2_bn(self.conv2(x)), 2)) x = x.</description>
    </item>
    
  </channel>
</rss>