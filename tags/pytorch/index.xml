<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pytorch on moskomule log</title>
    <link>https://mosko.tokyo/tags/pytorch/</link>
    <description>Recent content in Pytorch on moskomule log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <copyright>Written by Ryuichiro Hataya</copyright>
    <lastBuildDate>Fri, 01 Dec 2017 15:10:30 +0900</lastBuildDate>
    
	<atom:link href="https://mosko.tokyo/tags/pytorch/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>PyTorchでDQNを実装した</title>
      <link>https://mosko.tokyo/post/pytorch-dqn/</link>
      <pubDate>Fri, 01 Dec 2017 15:10:30 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/pytorch-dqn/</guid>
      <description>はじめに DQN(Deep Q Network)は Minh et al. 20151（以下論文）で登場した深層強化学習の先駆けです．Atariのゲームで非常に高い得点を修めるというパフォーマンスで有名になりました．
  9月頃に強化学習の勉強をした際に実装してみたのですが，一向に学習が進まず放置していたのですが，最近Implementing the Deep Q-Network 2を読み再開してみたところ，動いてしまったので，この記事を書くことになりました．
今回の実装はこちらにあります．
強化学習とは David Silver先生に聞きましょう．ただしこの講義では深層強化学習は扱われていません．
  Deep Q-Networkとは 論文を読みましょう．Q-Learningの応用で，複雑ではありませんが，学習を安定させるための工夫が各所にあるので見逃すと動かないようです．
 DQNの学習アルゴリズム．論文より．   実装について 画像の処理 DQNではAtariのゲームの画像をグレースケールにしてスタックするなどの処理がありますが，このあたりは各アルゴリズムをTensorFlowで実装し，公開しているOpen AI baselinesを一部変更して用いています．
 OpenCV2をPillowに変更した 画像のスタックの仕方をPyTorchに合わせて変更した．  また今回の改良ではtensorboard-pytorchを導入して，入力画像が正しいかを確認できるようにしました．
 ネットワーク Deepとは言えない気がしますが論文通りの構成です．何か工夫すると多少変わるのかもしれません．
class DQN(nn.Module): def __init__(self, output_size: int): super(DQN, self).__init__() self.feature = nn.Sequential( nn.Conv2d(4, 32, kernel_size=8, stride=4), nn.ReLU(inplace=True), nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(inplace=True), nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU(inplace=True)) self.fc = nn.</description>
    </item>
    
    <item>
      <title>Double Backpropagationについて</title>
      <link>https://mosko.tokyo/post/double-backprop/</link>
      <pubDate>Sun, 01 Oct 2017 19:40:24 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/double-backprop/</guid>
      <description>はじめに PyTorch v0.2では&amp;rdquo;Higher order gradients&amp;rdquo; (double backpropagation)がサポートされました．Chainerもv3においてこれがサポートされます．今回Chainer Meetupの資料を読んで雰囲気が分かったのでまとめました．
 Comparison of deep learning frameworks from a viewpoint of double backpropagation  Chainer v3  筆者は長くdouble backpropagationという名称から
\[\mathrm{loss}\longrightarrow \frac{\partial^2 \mathrm{loss}}{\partial x_i \partial x_j} \]
と思い込んでいました．そう思っているのでdocumentを読んでもいまいちよく分からない．ところが上に挙げた資料では，そうではなくて
\[\mathrm{loss}=g(f(x), \frac{\partial f(x)}{\partial x})\]
のような場合にも計算が出来る，ということなのだということが説明されていて救われました．
PyTorchの例 これで以上，でもよいのですが，PyTorchでの例を．
$x=1, y=x^3, z=y^2+\frac{dy}{dx}$をとして，$\frac{dz}{dx}|_{x=1}$を求めます．
&amp;gt;&amp;gt;&amp;gt; x = Variable(torch.Tensor([1]), requires_grad=True) &amp;gt;&amp;gt;&amp;gt; y = x ** 3 &amp;gt;&amp;gt;&amp;gt; grad_y, = autograd.grad(y, x, create_graph=True) &amp;gt;&amp;gt;&amp;gt; (grad_y + y ** 2).backward() &amp;gt;&amp;gt;&amp;gt; x.grad Variable containing: 12 [torch.</description>
    </item>
    
    <item>
      <title>PyTorchでCNN入門</title>
      <link>https://mosko.tokyo/post/pytorch_cnn/</link>
      <pubDate>Sat, 10 Jun 2017 15:37:12 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/pytorch_cnn/</guid>
      <description>CNNの概説 CNNは畳み込みニューラルネットワーク(convolutional neural network)の略です．CNNは四天王のひとりLeCun(1989)に始まり，2012年の一般物体認識のコンテスト(ILSVRC)で優勝しディープラーニングを一躍有名にしたAlexNet(Krizhevsky)を経て，現在の画像認識には欠かせないネットワークです．
畳み込み CNNでは畳み込み(convolution)という操作を行います．ここでは簡単のためにすべて2次元で考えます．
以下のように入力の行列とフィルタが与えられたときに，
\[I=ar+bs+tc+du+ev+fw+gz+hy+iz\]
を畳み込みと呼びます．本来画像認識の分野では$ax+by+cz+\cdots$を畳み込みと呼び，上記の演算は相関と呼ばれるようですが，CNNの文脈ではこれを畳み込みと呼ぶよう1なので慣例に倣います．
 左が入力の一部，右がフィルター．   入力に対して，この操作を同じフィルタをずらしながら適用していきます．下の図では上部の入力とフィルタの畳み込み結果を下の出力行列の各要素にする様子を書きました．こうして畳み込みによる出力が得られます．
 上が入力とフィルタ，下が出力．   畳み込みは画像の対応部分とフィルタとの内積を取ることですから，それらの関連ぐらいを見ていることになります（それ故に相関と呼ばれるのですが）．従って，出力は入力画像のフィルタとの関連度を凝縮したものになるわけです．以上の畳み込み（あるいは相関）自体はCNN以前から画像認識の分野で用いられてきましたが，CNNではフィルタ自体を誤差逆伝播法で学習していく点が従来とは異なります2．
上では入力，フィルタとも1枚ずつである場合を考えましたが，一般にそれらは複数枚あり，テンソルとして扱われます．この「枚数方向」の次元はチャネルと呼ばれます．特にRGB画像は3チャネルです．
複数チャネルの場合は，入力の各チャネルに対して同一のフィルタを適用し，その和をとります．従って，出力のチャネル数はフィルタ数と一致します．
プーリング CNNではその他にプーリングという操作を行う場合もあります．その中でもよく用いられる最大プーリング(max pooling)は下に示したように，領域内の最大値を取り出して出力とする操作です．画像認識では位置がずれた同じ物体も同じものとして認識したいので，この操作を加えて位置に対する不変性を向上させます．
 最大プーリング．上部が入力で下部が出力．   最大プーリングのほかに，平均値を用いるプーリングもあります．
用語 説明に用いる画像はこちらのもので，今までと異なり下が入力，上が出力です．
kernel 上記の畳み込みのフィルタやプーリングの領域のことをカーネルと呼ぶこともあります．
stride カーネルの動く際のステップです．プーリングの場合は領域幅と同じ幅で動かし，重複する範囲がないようにすることが多い気がします．
 stride=1   padding 畳み込み，プーリングを上記のように行った場合，出力は入力よりも小さくなります．入力の周りに「枠」を付けることで出力サイズを調整するのがpaddingです．「枠」を0で埋めるゼロパディングがしばしば用いられます．
 stride=1,padding=1   dilation カーネルにあける隙間の大きさです．プーリングの代わりにdilationを用いることもあるようです．
 stride=1,dilation=1   relu 活性化函数の一つで，“rectified linear unit”の略です．函数としては
\[\mathrm{relu}(x)=\max(0,x)\]
と極めて単純ですが，これがなければ現在のディープニューラルネットワーク時代はなかった，とも言えるような，強力な存在です．以前はsigmoid函数(S字状函数)，たとえば
\[\mathrm{sigmoid}(x)=\frac{1}{1+e^{-x}}\]
が用いられていましたが，ネットワークが深くなると勾配が消失する問題を抱えていました．
 sigmoid函数とrelu函数との比較   PyTorchにおけるCNN PyTorchの簡単なチュートリアルはこちらにあります．
コード中のFはnn.functionalのことです．
nn nn.Conv2dを用います．F.conv2dというものもありますが，こちらは自分で明示的にweight,biasのテンソルを用意し，必要であれば重みを更新しなくてはいけません．他方，nn.Conv2dであれば入出力のチャネル数およびカーネルの大きさを定めるだけです．今回は用いていませんが，上で説明したpadding,dilationを用いることもできます．
プーリングには，畳み込みのように更新すべきテンソルがないのでF.max_pool2dを用いても差はありません．</description>
    </item>
    
    <item>
      <title>PyTorchでディープラーニング</title>
      <link>https://mosko.tokyo/post/pytorch_tutorial/</link>
      <pubDate>Thu, 08 Jun 2017 16:33:50 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/pytorch_tutorial/</guid>
      <description>PyTorchとは PyTorchはFacebookの開発するPython上でのテンソル計算・自動微分ライブラリで，特にディープラーニングに必要な機能が充実しています．2017年の初頭に公開され，瞬く間にTensorflow, Keras, Caffeに続くディープラーニングライブラリとして人気を博すこととなりました．
Bonus: stars (not an indicator of usage, just proportional to how many people have landed on the GitHub page over the period). pic.twitter.com/IugHJqHSii
&amp;mdash; François Chollet (@fchollet) April 12, 2017  PyTorchはPreferred NetworkのディープラーニングライブラリChainerから影響を受けており，GoogleのTensorFlowやUniversité de MontréalのTheanoとは異なり，実行時に動的にグラフを構築するため，柔軟なコードを書くことができます．
PyTorchは，製品にも用いられているTensorFlowとは異なり，研究向けであることが明言されています．新機能の変更は多いものの，疎テンソルにいち早く対応するなど，最新の研究動向を追うにはよいのではないでしょうか．また，適当なレベルで書くことができて，素のTensorflowのように低レベルでもなく，Kerasの様に高度に抽象化されているわけでもなく，ラッパーによって書き方が多様でサンプルを見てもよく分からない，ということはないので，学びやすいと思います．
チュートリアル とりあえず動かせるようになるチュートリアルです．
インストール GPU環境は勿論，CPU環境でも動かすことができます．Linux，macOSの場合は 公式, Windowsの場合は Anaconda Cloudからインストールできます．
GPUを利用する場合，環境の設定が面倒なことが多いですがPyTorchでは特に設定せずにGPU対応版をダウンロードするとGPUが使えるようになるようです．
Tensor PyTorchの基本はテンソルを操作するTensorです．テンソルというと難しく聞こえますが，この場合は多次元配列と同義で，物理学のテンソルのような共変・反変を意識する必要はありません．慣例に従ってテンソル，と言う語を用います．
PyTorchにおけるTensorは端的に言えば「GPU上でも動くnumpy.ndarrayのようなもの」ですが，違いも多いので注意が必要です．例えば
import numpy as np import torch # PyTorch &amp;gt;&amp;gt;&amp;gt; np_tensor = np.zeros([1, 2, 3]) array([[[ 0., 0., 0.</description>
    </item>
    
    <item>
      <title>PyTorchはじめ</title>
      <link>https://mosko.tokyo/post/getting_started_pytorch/</link>
      <pubDate>Tue, 24 Jan 2017 15:15:55 +0900</pubDate>
      
      <guid>https://mosko.tokyo/post/getting_started_pytorch/</guid>
      <description>先日Facebookが PyTorch を公開していたので，早速試してみた．PyTorchは
 Tensors and Dynamic neural networks in Python with strong GPU acceleration.
 とのことで，TensorFlowやTheanoより，Chainerに似ている気がする．後発ということもあってか，ウェブページにある導入の説明が丁寧で，Linux，Python 3.5，conda，Cuda8.0なら
conda install pytorch torchvision cuda80 -c soumith  を叩くだけでよい．その下にはMNISTなどの例やJupyterのチュートリアルへのリンクがあるのも丁寧．ただ，ニューラルネットワークの知識に乏しくともレイヤーを重ねてscikit-learn風によしなにすればよいkerasよりは難しいが，その分柔軟に書けそう．メモリを大量消費するTensorFlowに較べて，GPUに対する負荷はかなり小さそう．
基本的にはnn.Moduleを継承してネットワークを定義する． 以下のコードはGithubに挙げた．
class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5, stride=1) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) self.conv2_bn = nn.BatchNorm2d(20) self.dense1 = nn.Linear(in_features=320, out_features=50) self.dense1_bn = nn.BatchNorm1d(50) self.dense2 = nn.Linear(50, 10) def forward(self, x): x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = F.relu(F.max_pool2d(self.conv2_bn(self.conv2(x)), 2)) x = x.</description>
    </item>
    
  </channel>
</rss>